{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963fc45d-1e26-4341-8f12-5444a25720da",
   "metadata": {},
   "source": [
    "# **Cross Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489150b-e815-49b7-b0a9-08eae0e319d9",
   "metadata": {},
   "source": [
    "**DEFINITION**: Cross-validation is a model validation technique used to assess how well a machine learning model will perform on unseen data by splitting the dataset into multiple training and testing subsets. This helps in evaluating the model's generalization ability and prevents overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da01849-0bf1-4f8b-b5c8-1fc8554847a0",
   "metadata": {},
   "source": [
    "### **ADVANTAGES**\n",
    "- It ensures that the model is tested on different subsets of data and improving its ability to generalize to unseen data.\n",
    "- Cross-validation is widely used in hyperparameter tuning (e.g., GridSearchCV, RandomizedSearchCV) to evaluate different hyperparameter combinations systematically and select the best-performing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64000ac-cac6-4211-9f3c-b580e431df1d",
   "metadata": {},
   "source": [
    "### **Disadvantages**  \n",
    "\n",
    "- Cross-validation is computationally expensive because the model is trained K times in K-Fold Cross-Validation, requiring significantly more computation than a simple train-test split. This makes it time-consuming, especially for large datasets or complex models like Random Forest, XGBoost, or Neural Networks, where training multiple times on different subsets increases execution time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1e297b-6883-409e-9b69-908196b1a657",
   "metadata": {},
   "source": [
    "### **Notes**\n",
    "\n",
    "**Not Suitable for Time-Series Data Without Modification**  \n",
    "   - Traditional K-Fold Cross-Validation randomly splits the data, which is **not ideal for time-series forecasting** since future data should never be used in training.  \n",
    "   - Instead, **Time-Series Cross-Validation (Rolling Window)** should be used.  \n",
    "\n",
    "**Variance in Scores**  \n",
    "   - If the dataset is **highly imbalanced or small**, performance scores can vary significantly across folds, leading to instability in evaluation.  \n",
    "   - **Stratified K-Fold** helps mitigate this issue in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f889e-6d0f-47bf-b7d3-46c4e23c8f69",
   "metadata": {},
   "source": [
    "### **Why Random K-fold Cross-Validation Can Be Misleading in Real-World Problems**\n",
    "\n",
    "- **Ref**: https://medium.com/towards-data-science/why-you-should-never-use-cross-validation-4360d42456ac\n",
    "1. **Overly Optimistic Estimates**  \n",
    "   - Cross-validation assumes that the training and test data have the same distribution, which is **rare in real-world scenarios**.  \n",
    "   - This can lead to **overestimating** model performance.  \n",
    "\n",
    "2. **Fails When Data Distribution Changes**  \n",
    "   - In real applications, new data often differs based on **time, location, or business factors**.  \n",
    "   - Standard cross-validation **does not account for this shift**, making it unreliable.  \n",
    "\n",
    "3. **Encourages Overfitting**  \n",
    "   - Since cross-validation trains and tests on similar distributions, it **favors models that memorize patterns** instead of generalizing well.  \n",
    "   - This can lead to selecting a model that **performs well in validation but fails in production**.  \n",
    "\n",
    "4. **Better Alternative: Group K-Fold**  \n",
    "   - Instead of randomly splitting data, **Group K-Fold** ensures that test data represents unseen groups (e.g., cities, months).  \n",
    "   - This provides a **more realistic performance estimate** and helps in choosing the right model.  \n",
    "\n",
    "#### **Conclusion:**  \n",
    "Cross-validation works well for general evaluation but **can be misleading when data shifts over time or across categories**. In real-world scenarios, use **Group K-Fold, Time Series Split, or other domain-specific validation strategies** to ensure better model selection and avoid over-optimistic results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65543c62-28d2-4c58-87a3-28caedfd9c40",
   "metadata": {},
   "source": [
    "## **Types of cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1700d7-3147-4420-bf9d-ccbe3ef3708a",
   "metadata": {},
   "source": [
    "- **Ref**: https://www.turing.com/kb/different-types-of-cross-validations-in-machine-learning-and-their-explanations\n",
    "\n",
    "## **1. K-Fold Cross-Validation**\n",
    "### **Concept**\n",
    "- The dataset is **randomly split** into **K equal parts** (**folds**).  \n",
    "- The model is trained on **K-1 folds** and tested on the **remaining fold**.  \n",
    "- This process repeats **K times**, ensuring each fold is used as a test set once.  \n",
    "- The **final performance score** is the **average of all K iterations**.  \n",
    "\n",
    "**Best for:** Large, balanced datasets.  \n",
    "**Not ideal for:** Imbalanced datasets.  \n",
    "\n",
    "### **Example Walkthrough**\n",
    "**Scenario:** Predicting student grades.  \n",
    "- **Dataset:** 5 students → **A, B, C, D, E**  \n",
    "- **K=5** (5 folds)  \n",
    "- **Each student is used as a test set once, while the others train the model.**  \n",
    "\n",
    "| **Iteration** | **Training Data (4 students)** | **Validation Data (1 student)** |\n",
    "|--------------|----------------------------|----------------------------|\n",
    "| **1** | B, C, D, E | A |\n",
    "| **2** | A, C, D, E | B |\n",
    "| **3** | A, B, D, E | C |\n",
    "| **4** | A, B, C, E | D |\n",
    "| **5** | A, B, C, D | E |\n",
    "\n",
    "**Each fold is used once as a test set**  \n",
    "\n",
    "### **Python Code**\n",
    "```python\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define K-Fold Cross-Validation (K=5)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "print(\"K-Fold Cross-validation scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Stratified K-Fold Cross-Validation**\n",
    "### **Concept**\n",
    "- **Same as K-Fold**, but ensures that **each fold maintains the same class distribution** as the original dataset.  \n",
    "- This prevents **class imbalance issues** (e.g., fraud detection, rare disease classification).  \n",
    "\n",
    "**Best for:** **Imbalanced datasets** (e.g., fraud detection, medical diagnosis).  \n",
    "**Not needed for:** **Already balanced datasets**.  \n",
    "\n",
    "### **Example Walkthrough**\n",
    "**Scenario:** Predicting disease presence (Imbalanced data: 3 Yes, 2 No).  \n",
    "- **Dataset:** 5 patients → **A(Yes), B(No), C(Yes), D(Yes), E(No)**  \n",
    "- **K=2** (2 folds)  \n",
    "- **Each fold keeps the same Yes/No ratio as the original dataset**  \n",
    "\n",
    "| **Iteration** | **Training Data** | **Validation Data** |\n",
    "|--------------|----------------------|----------------------|\n",
    "| **1** | A(Yes), B(No), D(Yes) | C(Yes), E(No) |\n",
    "| **2** | C(Yes), E(No) | A(Yes), B(No), D(Yes) |\n",
    "\n",
    "**Each fold maintains the same class balance.**  \n",
    "\n",
    "### **Python Code**\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform stratified cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(\"Stratified K-Fold Cross-validation scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Group K-Fold Cross-Validation**\n",
    "### **Concept**\n",
    "- Similar to K-Fold, but **ensures that entire groups of related samples are kept in the same fold**.  \n",
    "- Example: If you have multiple **records per patient**, **customer**, or **city**, this method prevents **data leakage**.  \n",
    "\n",
    "**Best for:** **Grouped datasets (e.g., customers, cities, patients, sensors).**  \n",
    "**Not ideal for:** **Completely independent observations.**  \n",
    "\n",
    "### **Example Walkthrough**\n",
    "**Scenario:** Predicting customer spending (Grouped by city).  \n",
    "- **Dataset:** 6 customers from 3 cities →  \n",
    "  **(A, B - New York), (C, D - LA), (E, F - Chicago)**  \n",
    "- **K=3** (3 folds)  \n",
    "- **Each fold keeps all customers from a city together**  \n",
    "\n",
    "| **Iteration** | **Training Data (Cities)** | **Validation Data (City)** |\n",
    "|--------------|----------------------------|----------------------------|\n",
    "| **1** | LA, Chicago | New York |\n",
    "| **2** | New York, Chicago | LA |\n",
    "| **3** | New York, LA | Chicago |\n",
    "\n",
    "**Prevents data leakage by keeping city-specific data in one fold.**  \n",
    "\n",
    "### **Python Code**\n",
    "```python\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Sample groups (e.g., different customers)\n",
    "groups = [1, 1, 2, 2, 3, 3, 4, 4, 5, 5]  # Assign a group ID to each sample\n",
    "\n",
    "# Define Group K-Fold\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "# Perform Group K-Fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=gkf, groups=groups, scoring='accuracy')\n",
    "\n",
    "print(\"Group K-Fold Cross-validation scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "### **4. Time Series Cross-Validation (Rolling Cross-Validation / Forward Chaining Method)**  \n",
    "#### **Concept**  \n",
    "Before diving into the rolling cross-validation technique, it's important to understand what **time-series data** is.  \n",
    "\n",
    "Time-series data consists of observations recorded **at different time points**, making it useful for understanding **patterns, trends, and seasonality**. Examples of time-series data include **stock prices, weather forecasts, economic indicators, and website traffic**.  \n",
    "\n",
    "Unlike regular datasets, where data points are independent of each other, **time-series data has a sequential nature**, meaning the past influences the future.  \n",
    "\n",
    "This characteristic makes standard cross-validation techniques unsuitable because **randomly splitting** time-series data (as done in K-Fold or Holdout CV) **breaks the chronological order** and may lead to unrealistic predictions.  \n",
    "\n",
    "### **How Rolling Cross-Validation Works**  \n",
    "Since the order of data is critical, **Time Series Cross-Validation (also known as Forward Chaining or Rolling Cross-Validation)** ensures that training always happens on **past data**, and validation happens on **future data**.  \n",
    "\n",
    "Instead of randomly splitting data, the dataset is divided **sequentially** into training and validation sets. Each iteration **expands the training set** by including more past data and tests the model on the next available time step.  \n",
    "\n",
    "#### **Steps:**  \n",
    "1. **Start with a small initial training set** and use it to make predictions on the next available time step.  \n",
    "2. **Expand the training set** by including that time step and move to the next period for validation.  \n",
    "3. **Repeat the process** until all time points are used for training and validation.  \n",
    "\n",
    "This method mimics **real-world forecasting**, where a model continuously updates as new data becomes available.  \n",
    "\n",
    "### **Example Walkthrough**  \n",
    "**Scenario:** Predicting monthly sales trends using historical sales data.  \n",
    "\n",
    "- **Dataset:** Monthly sales data from **January to June**.  \n",
    "- **Rolling Window Approach:** Each training set increases in size, and the test set consists of the next time period.  \n",
    "\n",
    "| **Iteration** | **Training Data (Months)** | **Validation Data (Next Month)** |  \n",
    "|--------------|----------------------|----------------------|  \n",
    "| **1** | January | February |  \n",
    "| **2** | January, February | March |  \n",
    "| **3** | January, February, March | April |  \n",
    "| **4** | January, February, March, April | May |  \n",
    "| **5** | January, February, March, April, May | June |  \n",
    "\n",
    "Ensures the model **never trains on future data** to prevent data leakage.  \n",
    "\n",
    "### **Python Code**  \n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic time-series data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Define Time Series Split (5 splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Define model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform time series cross-validation\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy = model.score(X_test, y_test)\n",
    "    print(f\"Test set: {test_index}, Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "### **Key Advantages of Time Series Cross-Validation**\n",
    "**Maintains chronological order** – prevents data leakage from future events.  \n",
    "**Mimics real-world forecasting** – models train on past data and predict future values.  \n",
    "**Improves model robustness** – evaluates how well the model generalizes over time.  \n",
    "\n",
    "### **When to Use It?**  \n",
    "**Best for:** Time-series forecasting tasks such as **stock market predictions, weather forecasting, energy consumption forecasting, and sales predictions.**  \n",
    "**Not suitable for:** Datasets with **independent observations** where order does not matter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d1360-e85c-4e3d-bb73-87d4ef905eab",
   "metadata": {},
   "source": [
    "---\n",
    "### **5. Holdout Cross-Validation (Train-Test Split)**  \n",
    "#### **Concept**  \n",
    "- The dataset is **randomly split** into **training (e.g., 70%) and testing (e.g., 30%) sets**.  \n",
    "- The model is trained on the training set and tested on the test set.  \n",
    "- This method is **fast**, but results depend on **how the split is made**.  \n",
    "\n",
    "**Best for:** Large datasets, quick evaluation  \n",
    "**Not ideal for:** Small datasets (risk of poor train-test distribution)  \n",
    "\n",
    "#### **Example Walkthrough**  \n",
    "**Scenario:** Predicting movie ratings  \n",
    "- **Dataset:** 10 movies  \n",
    "- **Split:** 7 movies for training, 3 for testing  \n",
    "\n",
    "| **Training Data** | **Validation Data** |  \n",
    "|------------------|------------------|  \n",
    "| Movie 1 - Movie 7 | Movie 8 - Movie 10 |  \n",
    "\n",
    "**Simple but depends on the split**  \n",
    "\n",
    "#### **Python Code**  \n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Leave-P-Out Cross-Validation (LPOCV)**  \n",
    "#### **Concept**  \n",
    "- **P samples are left out for validation**, while the remaining **N-P samples** are used for training.  \n",
    "- The process is repeated **for every possible combination** of test samples.  \n",
    "- Computationally expensive for large datasets.  \n",
    "\n",
    "**Best for:** Small datasets  \n",
    "**Not ideal for:** Large datasets (too slow)  \n",
    "\n",
    "#### **Example Walkthrough**  \n",
    "**Scenario:** Predicting employee performance  \n",
    "- **Dataset:** 5 employees **(A, B, C, D, E)**  \n",
    "- **P=2** (leave 2 out for validation)  \n",
    "\n",
    "| **Iteration** | **Training Data (3 employees)** | **Validation Data (2 employees)** |  \n",
    "|--------------|----------------------|----------------------|  \n",
    "| **1** | C, D, E | A, B |  \n",
    "| **2** | B, D, E | A, C |  \n",
    "| **3** | B, C, E | A, D |  \n",
    "| **4** | B, C, D | A, E |  \n",
    "\n",
    "**Exhaustive but slow for large datasets**  \n",
    "\n",
    "#### **Python Code**  \n",
    "```python\n",
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "# Define Leave-P-Out cross-validation (P=2)\n",
    "lpo = LeavePOut(p=2)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=lpo, scoring='accuracy')\n",
    "\n",
    "print(\"Leave-P-Out Cross-validation scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Leave-One-Out Cross-Validation (LOOCV)**  \n",
    "#### **Concept**  \n",
    "- A **special case of Leave-P-Out (P=1)**, where **only 1 sample is used as validation** at a time.  \n",
    "- The model is trained on **N-1 samples** and tested on **1 sample** in each iteration.  \n",
    "- **Very computationally expensive**, but uses **all available data** for training.  \n",
    "\n",
    "**Best for:** Small datasets  \n",
    "**Not ideal for:** Large datasets (too slow, too many iterations)  \n",
    "\n",
    "#### **Example Walkthrough**  \n",
    "**Scenario:** Predicting student's final exam score  \n",
    "- **Dataset:** 5 students **(A, B, C, D, E)**  \n",
    "- **Each student is tested once, trained on the rest**  \n",
    "\n",
    "| **Iteration** | **Training Data (4 students)** | **Validation Data (1 student)** |  \n",
    "|--------------|----------------------|----------------------|  \n",
    "| **1** | B, C, D, E | A |  \n",
    "| **2** | A, C, D, E | B |  \n",
    "| **3** | A, B, D, E | C |  \n",
    "| **4** | A, B, C, E | D |  \n",
    "\n",
    "**Good for small datasets, but slow**  \n",
    "\n",
    "#### **Python Code**  \n",
    "```python\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "# Define LOOCV\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=loo, scoring='accuracy')\n",
    "\n",
    "print(\"Leave-One-Out Cross-validation scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Monte Carlo Cross-Validation (Shuffle-Split)**  \n",
    "#### **Concept**  \n",
    "- The dataset is **randomly split into train/test sets multiple times** (e.g., 70-30 split).  \n",
    "- Unlike K-Fold, each fold **is chosen randomly** (not sequential).  \n",
    "- The results are **averaged** across iterations.  \n",
    "\n",
    "**Best for:** Large datasets  \n",
    "**Not ideal for:** Time-series data (random splits break time dependencies)  \n",
    "\n",
    "#### **Example Walkthrough**  \n",
    "**Scenario:** Predicting election outcomes  \n",
    "- **Dataset:** 10 states  \n",
    "- **Train-Test Split:** 7 states for training, 3 for testing (random split each time)  \n",
    "\n",
    "| **Iteration** | **Training Data (7 states)** | **Validation Data (3 states)** |  \n",
    "|--------------|----------------------|----------------------|  \n",
    "| **1** | A, B, C, D, E, F, G | H, I, J |  \n",
    "| **2** | B, D, E, F, G, H, I | A, C, J |  \n",
    "\n",
    "**Random splits avoid overfitting**  \n",
    "\n",
    "#### **Python Code**  \n",
    "```python\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# Define Shuffle-Split Cross-Validation\n",
    "ss = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=ss, scoring='accuracy')\n",
    "\n",
    "print(\"Monte Carlo Cross-validation scores:\", scores)\n",
    "print(\"Mean Accuracy:\", scores.mean())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2008b-458d-4ae9-8998-42e046f48967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
